{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frankzl/.envs/env36-ml/.venv/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/frankzl/datasets/mnist//train-images-idx3-ubyte.gz\n",
      "Extracting /home/frankzl/datasets/mnist//train-labels-idx1-ubyte.gz\n",
      "Extracting /home/frankzl/datasets/mnist//t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/frankzl/datasets/mnist//t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tools.datasets.mnist3 as mnist_tools\n",
    "import tools.datasets.mnistm as mnistm_tools \n",
    "import tools.semisup as semisup\n",
    "import numpy as np\n",
    "import architectures as arch\n",
    "from functools import partial\n",
    "\n",
    "import tools.visualization as vis\n",
    "import tools.updated_semisup as up\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "mnist_train_images, mnist_train_labels = mnist_tools.get_data('train')\n",
    "mnist_test_images, mnist_test_labels = mnist_tools.get_data('test')\n",
    "mnistm_train_images, mnistm_train_labels = mnistm_tools.get_data('train')\n",
    "mnistm_test_images, mnistm_test_labels = mnistm_tools.get_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled samples used per class\n",
    "# sup_per_class = 10\n",
    "sup_per_class = 1280\n",
    "sup_seed = -1\n",
    "# labeled samples per class per batch\n",
    "sup_per_batch = 100\n",
    "# unlabeled samples per batch\n",
    "unsup_batch_size = 1000\n",
    "unsup_samples = -1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "decay_steps = 9000\n",
    "decay_factor = 0.33\n",
    "logit_weight = 1.0\n",
    "\n",
    "max_steps = 2000\n",
    "eval_interval = 500\n",
    "\n",
    "log_dir = \"logs/training/mnist-mnistmK/model\"\n",
    "\n",
    "seed = 1\n",
    "\n",
    "IMAGE_SHAPE = mnist_tools.IMAGE_SHAPE\n",
    "NUM_LABELS = mnist_tools.NUM_LABELS\n",
    "\n",
    "# [10 (classes), 10 (samples), 28, 28, 1]\n",
    "sup_by_label = semisup.sample_by_label(mnist_train_images, mnist_train_labels,\n",
    "                        sup_per_class, NUM_LABELS, seed)\n",
    "\n",
    "visit_weight_envelope = \"linear\"\n",
    "visit_weight = 2\n",
    "visit_weight_envelope_steps = 1\n",
    "visit_weight_envelope_delay = 500\n",
    "\n",
    "walker_weight_envelope = \"linear\"\n",
    "walker_weight = 10\n",
    "walker_weight_envelope_steps = 1\n",
    "walker_weight_envelope_delay = 500\n",
    "\n",
    "TARGET_SHAPE = mnistm_tools.IMAGE_SHAPE\n",
    "TEST_SHAPE   = TARGET_SHAPE\n",
    "\n",
    "image_shape = IMAGE_SHAPE\n",
    "new_shape   = TARGET_SHAPE\n",
    "emb_size    = 128\n",
    "\n",
    "sampled_unsup_images = mnistm_train_images\n",
    "sampled_unsup_labels = mnistm_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dir = \"logs/training/mnist-mnistK-s40/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "net/conv1/weights:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "net/conv1/biases:0 (float32_ref 32) [32, bytes: 128]\n",
      "net/conv1_2/weights:0 (float32_ref 3x3x32x32) [9216, bytes: 36864]\n",
      "net/conv1_2/biases:0 (float32_ref 32) [32, bytes: 128]\n",
      "net/conv1_3/weights:0 (float32_ref 3x3x32x32) [9216, bytes: 36864]\n",
      "net/conv1_3/biases:0 (float32_ref 32) [32, bytes: 128]\n",
      "net/conv2_1/weights:0 (float32_ref 3x3x32x64) [18432, bytes: 73728]\n",
      "net/conv2_1/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "net/conv2_2/weights:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "net/conv2_2/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "net/conv2_3/weights:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "net/conv2_3/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "net/conv3/weights:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
      "net/conv3/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "net/conv3_2/weights:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
      "net/conv3_2/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "net/conv3_3/weights:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
      "net/conv3_3/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "net/fc1/weights:0 (float32_ref 1152x128) [147456, bytes: 589824]\n",
      "net/fc1/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "net/fully_connected/weights:0 (float32_ref 128x10) [1280, bytes: 5120]\n",
      "net/fully_connected/biases:0 (float32_ref 10) [10, bytes: 40]\n",
      "Total size of variables: 629642\n",
      "Total bytes of variables: 2518568\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import IntProgress, Layout\n",
    "from train import apply_envelope\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    unsup_data = up.create_input(sampled_unsup_images, sampled_unsup_labels, unsup_batch_size)\n",
    "    unsup_it   = unsup_data.make_initializable_iterator()\n",
    "    \n",
    "    t_unsup_images,t_unsup_labels = unsup_it.get_next()\n",
    "    \n",
    "    sup_data,sup_label = up.create_per_class_inputs_v2(sup_by_label, sup_per_batch)\n",
    "    sup_it             = sup_data.repeat().make_one_shot_iterator()\n",
    "    sup_it_label       = sup_label.repeat().make_one_shot_iterator()\n",
    "    \n",
    "    # Apply augmentation\n",
    "    def _random_invert(inputs, _):\n",
    "        randu = tf.random_uniform(\n",
    "            shape=[sup_per_batch * NUM_LABELS], minval=0.,\n",
    "            maxval=1.,\n",
    "            dtype=tf.float32)\n",
    "        randu = tf.cast(tf.less(randu, 0.5), tf.float32)\n",
    "        randu = tf.expand_dims(randu, 1)\n",
    "        randu = tf.expand_dims(randu, 1)\n",
    "        randu = tf.expand_dims(randu, 1)\n",
    "        inputs = tf.cast(inputs, tf.float32)\n",
    "        return tf.abs(inputs - 255 * randu)\n",
    "\n",
    "    augmentation_function = _random_invert\n",
    "    \n",
    "    model_func = partial(\n",
    "        arch.svhn_model,\n",
    "        new_shape=new_shape,\n",
    "        augmentation_function=augmentation_function,\n",
    "        img_shape=image_shape,\n",
    "        emb_size=emb_size\n",
    "    )\n",
    "    \n",
    "    model = semisup.SemisupModel(model_func, NUM_LABELS, IMAGE_SHAPE,\n",
    "                                 test_in=tf.placeholder(np.float32, [None] + TEST_SHAPE, 'test_in')\n",
    "                                )\n",
    "    \n",
    "    t_sup_images, t_sup_labels = tf.concat( sup_it.get_next(), 0), tf.concat( sup_it_label.get_next(), 0)\n",
    "    \n",
    "    t_sup_emb   = model.image_to_embedding(t_sup_images)\n",
    "    t_unsup_emb = model.image_to_embedding(t_unsup_images)\n",
    "    \n",
    "    t_sup_logit = model.embedding_to_logit(t_sup_emb)\n",
    "    \n",
    "    visit_weight = apply_envelope(\n",
    "        type = visit_weight_envelope,\n",
    "        step = model.step,\n",
    "        final_weight = visit_weight,\n",
    "        growing_steps = visit_weight_envelope_steps,\n",
    "        delay = visit_weight_envelope_delay\n",
    "    )\n",
    "    \n",
    "    walker_weight = apply_envelope(\n",
    "        type = walker_weight_envelope,\n",
    "        step = model.step,\n",
    "        final_weight = walker_weight,\n",
    "        growing_steps = walker_weight_envelope_steps,\n",
    "        delay = walker_weight_envelope_delay\n",
    "    )\n",
    "    \n",
    "    tf.summary.scalar(\"Weights_Visit\", visit_weight)\n",
    "    tf.summary.scalar(\"Weight_Walker\", walker_weight)\n",
    "    \n",
    "    model.add_logit_loss(t_sup_logit, t_sup_labels, weight=logit_weight)\n",
    "    \n",
    "    #model.add_semisup_loss(t_sup_emb, t_unsup_emb, t_sup_labels, visit_weight=visit_weight, walker_weight=walker_weight)\n",
    "    equality_matrix = tf.equal(tf.reshape(t_sup_labels, [-1, 1]), t_sup_labels)\n",
    "    equality_matrix = tf.cast(equality_matrix, tf.float32)\n",
    "    p_target = (equality_matrix / tf.reduce_sum(\n",
    "        equality_matrix, [1], keepdims=True))\n",
    "\n",
    "    embedding_dim = t_sup_emb.shape[1]\n",
    "    reshaped_semb = tf.reshape( t_sup_emb, [-1, 1, embedding_dim] )\n",
    "    reshaped_uemb = tf.reshape( t_unsup_emb, [-1, 1, embedding_dim] )\n",
    "    \n",
    "    stacked_semb = tf.stack(unsup_batch_size*[t_sup_emb], 1)\n",
    "    stacked_uemb = tf.stack(sup_per_batch*NUM_LABELS*[t_unsup_emb], 1)\n",
    "    \n",
    "    uemb_T = tf.transpose(stacked_uemb, perm=[1,0,2])\n",
    "     \n",
    "    sigma = 40\n",
    "    pairwise_dist = (stacked_semb - uemb_T)#, axis=2)\n",
    "    pairwise_norm = tf.norm( pairwise_dist, axis=2)\n",
    "    pairwise_sq   = tf.square(pairwise_norm)\n",
    "    \n",
    "    match_ab   = tf.exp(- tf.divide( pairwise_sq, tf.constant(2*sigma**2, dtype=tf.float32)), name='match_ab')\n",
    "    \n",
    "    p_ab = tf.nn.softmax(match_ab, name='p_ab')\n",
    "    p_ba = tf.nn.softmax(tf.transpose(match_ab), name='p_ba')\n",
    "    p_aba = tf.matmul(p_ab, p_ba, name='p_aba')\n",
    "\n",
    "    model.create_walk_statistics(p_aba, equality_matrix)\n",
    "    \n",
    "    loss_aba = tf.losses.softmax_cross_entropy(\n",
    "        p_target,\n",
    "        tf.log(1e-8 + p_aba),\n",
    "        weights=walker_weight,\n",
    "        scope='loss_aba')\n",
    "    \n",
    "    mab_dt, pab_dt, paba_dt, semb_dt, uemb_dt = tf.gradients([loss_aba], [match_ab, p_ab, p_aba, t_sup_emb, t_unsup_emb])\n",
    "    \n",
    "    model.add_visit_loss(p_ab, visit_weight)\n",
    "\n",
    "    tf.summary.scalar('Loss_aba', loss_aba)\n",
    "    \n",
    "    t_learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate,\n",
    "        model.step,\n",
    "        decay_steps,\n",
    "        decay_factor,\n",
    "        staircase = True\n",
    "    )\n",
    "    \n",
    "    train_op = model.create_train_op(t_learning_rate)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(store_dir, graph)\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/training/mnist-mnistmK/model'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39ae637d82c40f18e845b4871d8f2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, layout=Layout(width='100%'), max=1502)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-501\n",
      "Time left: 5:22:58s\n",
      "Step: 600\n",
      "[[100   0  17   9   6   3  20   3  11   6]\n",
      " [  3 112  20   9  35   9  12  12  20   2]\n",
      " [  7   1 156  16   4   2   5  14  12   2]\n",
      " [  3   2  25 125   3   9   3   5  27   5]\n",
      " [  1   5  17   8 130   7  14  17  12   6]\n",
      " [  2   0   3  22   4 115  10   1  20   2]\n",
      " [ 10   5   8  11   4  13 110   1  14   2]\n",
      " [  1   3  14  12  11   2   3 143  14   2]\n",
      " [  5   2  22  14   5   9  15   9 106   5]\n",
      " [  4   3  10  25  24  12   9  26   6  75]]\n",
      "Test error: 41.40 %\n",
      "Loss: 82.7200698852539\n",
      "\n",
      "saving model - step 600\n",
      "Time left: 4:43:17s\n",
      "Step: 700\n",
      "[[ 94   4  11   5  11   5  21   4  16   4]\n",
      " [  1 135  22   6  13  12  22   9  12   2]\n",
      " [  6   7 154  14   6   2   5   8  14   3]\n",
      " [  2   8  19 113   4  21   5   7  24   4]\n",
      " [  0   6  16   5 131   4  17  14  14  10]\n",
      " [  0   2   3  10   4 119  18   2  19   2]\n",
      " [  5   6  10   6   9  13 110   3  15   1]\n",
      " [  0  14  16   7   6   3   5 134  16   4]\n",
      " [  2   9  15  11   5   8  18   9 112   3]\n",
      " [  4   9   8  21  31  16   7  15  13  70]]\n",
      "Test error: 41.40 %\n",
      "Loss: 82.21905517578125\n",
      "\n",
      "saving model - step 700\n",
      "Time left: 4:04:32s\n",
      "Step: 800\n",
      "[[102   9  11   3   7   4   9   3  22   5]\n",
      " [  2 140  13   8  15  13  11  15  13   4]\n",
      " [  7   5 150  17   6   5   4   9  14   2]\n",
      " [  5   9  13 119   4  17   4   5  30   1]\n",
      " [  3  11  13   8 123   9   9  14  15  12]\n",
      " [  0   5   2  17   4 119  13   3  14   2]\n",
      " [  8  12  12   7   7  16  97   5  13   1]\n",
      " [  1  13  13  11   7   8   3 130  12   7]\n",
      " [  6   8   8  13   6  14  13   7 114   3]\n",
      " [  8   9   8  23  21  16   6  17  12  74]]\n",
      "Test error: 41.60 %\n",
      "Loss: 82.04190826416016\n",
      "\n",
      "saving model - step 800\n",
      "Time left: 3:41:59s\n",
      "Step: 900\n",
      "[[ 98   9  16   6  14   2   9   4  13   4]\n",
      " [  2 137  14  12  16  10   8  19  13   3]\n",
      " [  5   5 150  19   8   7   3   7  12   3]\n",
      " [  5  11  14 123   6  12   5   6  23   2]\n",
      " [  2  10  11  12 133   8   6  19   8   8]\n",
      " [  0   4   3  26   7 106  16   4  11   2]\n",
      " [  6  11  13  11  14  11  94   8  10   0]\n",
      " [  1  12  15  19  10   7   5 127   6   3]\n",
      " [  6  10  11  18   7   8  16   8 106   2]\n",
      " [  5  10   9  26  31  14   5  16  12  66]]\n",
      "Test error: 43.00 %\n",
      "Loss: 81.8987808227539\n",
      "\n",
      "saving model - step 900\n",
      "Time left: 3:01:20s\n",
      "Step: 1000\n",
      "[[ 96   7  19  10  10   1   9   6   6  11]\n",
      " [  2 133  13  16  13  11   9  24  10   3]\n",
      " [  3   7 156  22   7   4   1   6  10   3]\n",
      " [  5  12  13 125   6  11   4   7  19   5]\n",
      " [  1  12  12  15 124   9   6  19   8  11]\n",
      " [  1   5   4  32   5 104  14   2  10   2]\n",
      " [  5  11  15  12  11  15  90   7  10   2]\n",
      " [  3  12  11  23   7   7   4 130   6   2]\n",
      " [  4   8  16  22   6   9  13  12  97   5]\n",
      " [  4   7  10  33  18  13   7  17   6  79]]\n",
      "Test error: 43.30 %\n",
      "Loss: 81.80545043945312\n",
      "\n",
      "saving model - step 1000\n",
      "Time left: 2:25:28s\n",
      "Step: 1100\n",
      "[[104   6  16  10   6   1   9   7   5  11]\n",
      " [  5 134  15  15  11   5   9  24  13   3]\n",
      " [  7   5 152  20   4   5   5   8   8   5]\n",
      " [  7  14  13 122   3  11   5   6  20   6]\n",
      " [  3  14  11  13 123  10   7  16   6  14]\n",
      " [  3   7   4  32   1 102  14   1  12   3]\n",
      " [ 11  10  14  13   7  15  89   7  10   2]\n",
      " [  5  12  12  22   6   6   4 126   5   7]\n",
      " [ 10  10  18  23   6   5  13  10  92   5]\n",
      " [  8   7  14  26  18   9   8  14   7  83]]\n",
      "Test error: 43.65 %\n",
      "Loss: 81.74559783935547\n",
      "\n",
      "saving model - step 1100\n",
      "Time left: 1:50:17s\n",
      "Step: 1200\n",
      "[[101   8  13   7   8   6  14   4   5   9]\n",
      " [  2 141  15  11  15  13  10  15  10   2]\n",
      " [  8  10 153  13   4   8   6   5   9   3]\n",
      " [  4  15  15 111   3  19  10   4  18   8]\n",
      " [  5  19  12  10 122  12   9  11   8   9]\n",
      " [  3   8   4  16   2 115  18   2   8   3]\n",
      " [  6   9  12   9   8  20  99   5   9   1]\n",
      " [  4  16  17  14   9  12   8 119   1   5]\n",
      " [  8  15  20  17   8  12  17   8  85   2]\n",
      " [ 10  10  17  18  29  18  12  12   4  64]]\n",
      "Test error: 44.50 %\n",
      "Loss: 81.65607452392578\n",
      "\n",
      "saving model - step 1200\n",
      "Time left: 1:16:46s\n",
      "Step: 1300\n",
      "[[103   9  12   8   8   4   9   5   8   9]\n",
      " [  5 151  12  14   6  10   5  16  11   4]\n",
      " [  6  13 152  15   4   6   6   4   8   5]\n",
      " [  6  18  12 119   4  14   6   3  13  12]\n",
      " [  7  23  12  10 118  12   6   9   4  16]\n",
      " [  4  15   4  21   1 106  14   2   9   3]\n",
      " [  8  15  12  11   5  14  93   4  11   5]\n",
      " [  7  17  17  20   9   6   7 113   1   8]\n",
      " [ 11  23  19  18   7   6   9   9  85   5]\n",
      " [  9  14  15  17  20  10   9  13   4  83]]\n",
      "Test error: 43.85 %\n",
      "Loss: 81.6565933227539\n",
      "\n",
      "saving model - step 1300\n",
      "Time left: 0:36:25s\n",
      "Step: 1400\n",
      "[[ 98   7  12  10   7   3  14   4   8  12]\n",
      " [  7 140  15  14   2   9  14  18  14   1]\n",
      " [  8   4 153  17   3   4   9   6  11   4]\n",
      " [  6  11  13 123   2  13   9   3  20   7]\n",
      " [  7  20  13  10 104  10  17  11   8  17]\n",
      " [  2   7   5  22   2 108  18   2   9   4]\n",
      " [  5  10  12  12   3  12 100   5  13   6]\n",
      " [ 10  10  18  21   3   6  12 112   4   9]\n",
      " [ 12  12  19  19   6   8  18   7  87   4]\n",
      " [ 11  12  19  19  16  12  13  15   5  72]]\n",
      "Test error: 45.15 %\n",
      "Loss: 81.59916687011719\n",
      "\n",
      "saving model - step 1400\n",
      "Time left: 0:00:43s\n",
      "Step: 1500\n",
      "[[ 95   5  16  10   9   7  16   3   6   8]\n",
      " [ 11 130  18  14   4  12  16  13  13   3]\n",
      " [ 11   5 157  14   2   7   6   5   9   3]\n",
      " [  6  10  14 123   4  13   7   4  20   6]\n",
      " [  6  17  13  12 105  10  18  11   7  18]\n",
      " [  2   6   5  20   1 114  18   1   9   3]\n",
      " [  5   8  11  12   2  17 102   3  11   7]\n",
      " [  9   9  22  19   2   9  11 114   3   7]\n",
      " [ 13  10  23  16   7  12  18   8  82   3]\n",
      " [  8  13  19  20  16  13  13  14   7  71]]\n",
      "Test error: 45.35 %\n",
      "Loss: 81.59049987792969\n",
      "\n",
      "saving model - step 1500\n"
     ]
    }
   ],
   "source": [
    "test_images = mnistm_test_images[:2000]\n",
    "test_labels = mnistm_test_labels[:2000]\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "max_steps = 1502\n",
    "f = IntProgress(min=0, max=max_steps, layout= Layout(width=\"100%\")) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "sesh = tf.Session(graph = graph, config=tf.ConfigProto(intra_op_parallelism_threads=2, allow_soft_placement=True))\n",
    "\n",
    "eval_interval = 100\n",
    "\n",
    "p_aba_list = []\n",
    "match_ab_list = []\n",
    "t_sup_emb_list = []\n",
    "t_unsup_emb_list = []\n",
    "\n",
    "model_checkpoint = f\"{log_dir}-{501}\"\n",
    "\n",
    "with sesh as sess:\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=30)\n",
    "    saver.restore(sess, model_checkpoint)\n",
    "    \n",
    "    sess.run(unsup_it.initializer)\n",
    "    \n",
    "    epoch = 0\n",
    "    \n",
    "    for step in range(501, max_steps):\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            loss, _, summaries = sess.run([model.train_loss, train_op, summary_op])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            sess.run(unsup_it.initializer)\n",
    "            epoch += 1\n",
    "            \n",
    "            loss, _, summaries = sess.run([model.train_loss, train_op, summary_op])\n",
    "           \n",
    "        f.value = step\n",
    "        f.description = f\"Ep{epoch}:{step}/{max_steps}\"\n",
    "        if (step) % eval_interval == 0:\n",
    "            \n",
    "            t1 = time.time()\n",
    "            print(f\"Time left: {datetime.timedelta(seconds=int((t1-t0)*(max_steps-step)))}s\")\n",
    "            \n",
    "            print('Step: %d' % step)\n",
    "            test_pred = model.classify(test_images).argmax(-1)\n",
    "            conf_mtx = semisup.confusion_matrix(test_labels, test_pred, NUM_LABELS)\n",
    "            test_err = (test_labels != test_pred).mean() * 100\n",
    "            print(conf_mtx)\n",
    "            print('Test error: %.2f %%' % test_err)\n",
    "            print(f'Loss: {loss}')\n",
    "            print()\n",
    "    \n",
    "            test_summary = tf.Summary(\n",
    "                value=[tf.Summary.Value(\n",
    "                    tag='Test Err', simple_value=test_err)])\n",
    "    \n",
    "            summary_writer.add_summary(summaries, step)\n",
    "            summary_writer.add_summary(test_summary, step)\n",
    "\n",
    "            print(f\"saving model - step {step}\")\n",
    "            saver.save(sess, store_dir, global_step=model.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-1\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-101\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-201\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-301\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-401\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-501\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-601\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-701\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-801\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-901\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-1001\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-1101\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-1201\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-1301\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-1401\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-1501\n"
     ]
    }
   ],
   "source": [
    "all_steps = list(range(1,1502,100))\n",
    "\n",
    "p_aba_list = []\n",
    "match_ab_list = []\n",
    "p_ab_list = []\n",
    "semb_list = []\n",
    "uemb_list = []\n",
    "slabel_list = []\n",
    "ulabel_list = []\n",
    "\n",
    "for train_step in all_steps:\n",
    "    if train_step <= 501:\n",
    "        model_checkpoint = f\"{log_dir}-{train_step}\"\n",
    "    else:\n",
    "        model_checkpoint = f\"{store_dir}-{train_step}\"\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_checkpoint)\n",
    "        \n",
    "        sess.run(unsup_it.initializer)\n",
    "        \n",
    "        pab, slabel, ulabel, ab, aba, semb, uemb = sess.run([p_ab, t_sup_labels, t_unsup_labels, match_ab, p_aba, t_sup_emb, t_unsup_emb])\n",
    "        p_aba_list.append(aba)\n",
    "        match_ab_list.append(ab)\n",
    "        p_ab_list.append(pab)\n",
    "        semb_list.append(semb)\n",
    "        uemb_list.append(uemb)\n",
    "        slabel_list.append(slabel)\n",
    "        ulabel_list.append(ulabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-601\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    model_checkpoint = f\"{store_dir}-{601}\"\n",
    "    saver.restore(sess, model_checkpoint)\n",
    "    sess.run(unsup_it.initializer)\n",
    "    \n",
    "    laba, mabgr, pabgr, pabagr, sgr, ugr = sess.run([loss_aba, mab_dt, pab_dt, paba_dt, semb_dt, uemb_dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00528175, -0.00561872, -0.00520749, ...,  0.00059999,\n",
       "         0.00059999,  0.00059999],\n",
       "       [-0.00529621, -0.00560133, -0.00522016, ...,  0.00059999,\n",
       "         0.00059999,  0.00059999],\n",
       "       [-0.00530337, -0.00563986, -0.00519285, ...,  0.00059999,\n",
       "         0.00059999,  0.00059999],\n",
       "       ...,\n",
       "       [ 0.00059999,  0.00059999,  0.00059999, ..., -0.00523213,\n",
       "        -0.00545448, -0.00540969],\n",
       "       [ 0.00059999,  0.00059999,  0.00059999, ..., -0.00523463,\n",
       "        -0.00544493, -0.00540712],\n",
       "       [ 0.00059999,  0.00059999,  0.00059999, ..., -0.00528052,\n",
       "        -0.00549948, -0.00536054]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pabagr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.31671072e-05,  2.22829549e-05,  3.88521403e-06, ...,\n",
       "         3.97997937e-05,  2.60515644e-05, -1.02260929e-05],\n",
       "       [ 3.20304352e-05,  2.11506140e-05,  2.69929842e-06, ...,\n",
       "         3.86981919e-05,  2.49356308e-05, -1.14620943e-05],\n",
       "       [ 3.28725364e-05,  2.18775967e-05,  3.56622877e-06, ...,\n",
       "         3.94613635e-05,  2.56402636e-05, -1.04909595e-05],\n",
       "       ...,\n",
       "       [-8.02024297e-05,  2.86057038e-05, -3.66997119e-05, ...,\n",
       "        -1.15878793e-05, -5.89669944e-05,  2.49999139e-05],\n",
       "       [-7.96724198e-05,  2.90179014e-05, -3.62238206e-05, ...,\n",
       "        -1.11469562e-05, -5.85033558e-05,  2.54311744e-05],\n",
       "       [-8.08653858e-05,  2.87733856e-05, -3.68884212e-05, ...,\n",
       "        -1.14469440e-05, -5.83730289e-05,  2.45909032e-05]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pabgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.51997567e-08,  5.30075681e-08,  1.75683752e-08, ...,\n",
       "         8.28770226e-08,  5.79718034e-08, -1.34038212e-08],\n",
       "       [ 3.70654334e-08,  2.02222559e-08, -1.61795572e-08, ...,\n",
       "         5.50345831e-08,  3.00880885e-08, -4.82694666e-08],\n",
       "       [ 7.75299043e-08,  5.55734836e-08,  2.91538385e-08, ...,\n",
       "         9.63606581e-08,  6.20624547e-08, -3.77756582e-09],\n",
       "       ...,\n",
       "       [-1.53317416e-07,  6.22706153e-08, -6.39316724e-08, ...,\n",
       "        -1.10663221e-08, -9.49409724e-08,  6.75445548e-08],\n",
       "       [-1.96473820e-07,  4.34731184e-08, -8.87426950e-08, ...,\n",
       "        -3.11478203e-08, -1.15436286e-07,  4.21994919e-08],\n",
       "       [-1.54327836e-07,  5.69168535e-08, -7.82367806e-08, ...,\n",
       "        -2.56784300e-08, -1.49675856e-07,  4.48925306e-08]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mabgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.73160100e-08,  1.43034535e-08,  9.16143694e-09, ...,\n",
       "        -1.38655025e-08, -7.06721293e-10, -3.01570182e-08],\n",
       "       [ 3.01983434e-08,  2.41971261e-08, -1.71434849e-08, ...,\n",
       "        -5.27470900e-10,  6.03642292e-09,  4.04439788e-08],\n",
       "       [-1.86297182e-08,  3.54174041e-08,  1.08918679e-08, ...,\n",
       "        -1.76803141e-08, -1.73815522e-08, -2.46733158e-08],\n",
       "       ...,\n",
       "       [ 3.09021608e-08, -4.55078919e-10,  1.29072335e-08, ...,\n",
       "        -1.28435185e-09,  2.12495110e-09,  2.38004425e-08],\n",
       "       [ 1.80136139e-09, -1.01491429e-08,  5.80294923e-09, ...,\n",
       "         1.83752409e-08,  2.34520225e-09,  1.82092719e-09],\n",
       "       [ 2.03308250e-08, -4.96763608e-09,  1.00391846e-08, ...,\n",
       "        -2.35994890e-09,  4.96717723e-10,  1.83357010e-08]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_uemb_list     = []\n",
    "sorted_match_ab_list = []\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "for idx in range(len(uemb_list)):\n",
    "    sorted_uemb_list.append(uemb_list[idx][ulabel_list[idx].argsort()])\n",
    "    sorted_match_ab_list.append(match_ab_list[idx][:,ulabel_list[idx].argsort()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frankzl/.envs/env36-ml/.venv/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14d6efe6f5f436f810ed43a6c8f7960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=901, description='Train Step', max=1501, min=1, step=100), RadioButtons(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(step, array)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from ipywidgets import IntSlider, RadioButtons\n",
    "from IPython.html.widgets import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_img(array, step):\n",
    "    if array == 0:\n",
    "        a = p_aba_list\n",
    "    elif array == 1:\n",
    "        a = match_ab_list\n",
    "    elif array == 2:\n",
    "        a = sorted_match_ab_list\n",
    "    else:\n",
    "        a = p_ab_list \n",
    "            \n",
    "            \n",
    "    fig = plt.figure()\n",
    "    plt.imshow(a[(step-1)//100])\n",
    "    plt.colorbar()\n",
    "\n",
    "interact(\n",
    "    lambda step, array: plot_img(array, step),\n",
    "    step = IntSlider(\n",
    "        value=901,\n",
    "        min=1,\n",
    "        max=1501,\n",
    "        step=100,\n",
    "        description='Train Step',\n",
    "        continuous_update=True,\n",
    "        orientation='horizontal',\n",
    "    ),\n",
    "    array = RadioButtons(\n",
    "        options=[('p_aba', 0), (\"match_ab\", 1), (\"sorted_match_ab\", 2), (\"p_ab\", 3)],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "\n",
    "my_semb = semb_list[idx]\n",
    "my_uemb = uemb_list[idx]\n",
    "\n",
    "embedding_dim = my_semb.shape[1]\n",
    "\n",
    "reshaped_semb = my_semb.reshape((-1, 1, embedding_dim))\n",
    "reshaped_uemb = my_uemb.reshape((-1, 1, embedding_dim))\n",
    "\n",
    "repeat_semb = np.repeat(reshaped_semb, unsup_batch_size, axis=1)\n",
    "repeat_uemb = np.repeat(reshaped_uemb, sup_per_batch*NUM_LABELS, axis=1)\n",
    "\n",
    "uemb_T = np.transpose(repeat_uemb, axes=[1,0,2])\n",
    "\n",
    "sigma = 40\n",
    "my_match_ab = np.exp(-(np.linalg.norm(repeat_semb - uemb_T, axis=2)**2)/(2*sigma**2))\n",
    "\n",
    "\n",
    "from sklearn.utils.extmath import softmax\n",
    "plt.imshow(softmax(my_match_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "\n",
    "with g2.as_default():\n",
    "    \n",
    "    x = tf.ones([1])\n",
    "    t = tf.gradients([x**2], [x])\n",
    "\n",
    "\n",
    "with tf.Session(graph = g2) as sesh:\n",
    "    print(sesh.run(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
