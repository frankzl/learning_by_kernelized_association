{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frankzl/.envs/env36-ml/.venv/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/frankzl/datasets/mnist//train-images-idx3-ubyte.gz\n",
      "Extracting /home/frankzl/datasets/mnist//train-labels-idx1-ubyte.gz\n",
      "Extracting /home/frankzl/datasets/mnist//t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/frankzl/datasets/mnist//t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tools.datasets.mnist3 as mnist_tools\n",
    "import tools.datasets.mnistm as mnistm_tools \n",
    "import tools.semisup as semisup\n",
    "import numpy as np\n",
    "import architectures as arch\n",
    "from functools import partial\n",
    "\n",
    "import tools.visualization as vis\n",
    "import tools.updated_semisup as up\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "mnist_train_images, mnist_train_labels = mnist_tools.get_data('train')\n",
    "mnist_test_images, mnist_test_labels = mnist_tools.get_data('test')\n",
    "mnistm_train_images, mnistm_train_labels = mnistm_tools.get_data('train')\n",
    "mnistm_test_images, mnistm_test_labels = mnistm_tools.get_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled samples used per class\n",
    "# sup_per_class = 10\n",
    "sup_per_class = 1280\n",
    "sup_seed = -1\n",
    "# labeled samples per class per batch\n",
    "sup_per_batch = 100\n",
    "# unlabeled samples per batch\n",
    "unsup_batch_size = 1000\n",
    "unsup_samples = -1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "decay_steps = 9000\n",
    "decay_factor = 0.33\n",
    "logit_weight = 1.0\n",
    "\n",
    "max_steps = 2000\n",
    "eval_interval = 500\n",
    "\n",
    "log_dir = \"logs/training/mnist-mnistmK/model\"\n",
    "\n",
    "seed = 1\n",
    "\n",
    "IMAGE_SHAPE = mnist_tools.IMAGE_SHAPE\n",
    "NUM_LABELS = mnist_tools.NUM_LABELS\n",
    "\n",
    "# [10 (classes), 10 (samples), 28, 28, 1]\n",
    "sup_by_label = semisup.sample_by_label(mnist_train_images, mnist_train_labels,\n",
    "                        sup_per_class, NUM_LABELS, seed)\n",
    "\n",
    "visit_weight_envelope = \"linear\"\n",
    "visit_weight = 2\n",
    "visit_weight_envelope_steps = 1\n",
    "visit_weight_envelope_delay = 500\n",
    "\n",
    "walker_weight_envelope = \"linear\"\n",
    "walker_weight = 10\n",
    "walker_weight_envelope_steps = 1\n",
    "walker_weight_envelope_delay = 500\n",
    "\n",
    "TARGET_SHAPE = mnistm_tools.IMAGE_SHAPE\n",
    "TEST_SHAPE   = TARGET_SHAPE\n",
    "\n",
    "image_shape = IMAGE_SHAPE\n",
    "new_shape   = TARGET_SHAPE\n",
    "emb_size    = 128\n",
    "\n",
    "sampled_unsup_images = mnistm_train_images\n",
    "sampled_unsup_labels = mnistm_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dir = \"logs/training/mnist-mnistK-s0.5/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "net/conv1/weights:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "net/conv1/biases:0 (float32_ref 32) [32, bytes: 128]\n",
      "net/conv1_2/weights:0 (float32_ref 3x3x32x32) [9216, bytes: 36864]\n",
      "net/conv1_2/biases:0 (float32_ref 32) [32, bytes: 128]\n",
      "net/conv1_3/weights:0 (float32_ref 3x3x32x32) [9216, bytes: 36864]\n",
      "net/conv1_3/biases:0 (float32_ref 32) [32, bytes: 128]\n",
      "net/conv2_1/weights:0 (float32_ref 3x3x32x64) [18432, bytes: 73728]\n",
      "net/conv2_1/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "net/conv2_2/weights:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "net/conv2_2/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "net/conv2_3/weights:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "net/conv2_3/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "net/conv3/weights:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
      "net/conv3/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "net/conv3_2/weights:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
      "net/conv3_2/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "net/conv3_3/weights:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
      "net/conv3_3/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "net/fc1/weights:0 (float32_ref 1152x128) [147456, bytes: 589824]\n",
      "net/fc1/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "net/fully_connected/weights:0 (float32_ref 128x10) [1280, bytes: 5120]\n",
      "net/fully_connected/biases:0 (float32_ref 10) [10, bytes: 40]\n",
      "Total size of variables: 629642\n",
      "Total bytes of variables: 2518568\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import IntProgress, Layout\n",
    "from train import apply_envelope\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    unsup_data = up.create_input(sampled_unsup_images, sampled_unsup_labels, unsup_batch_size)\n",
    "    unsup_it   = unsup_data.make_initializable_iterator()\n",
    "    \n",
    "    t_unsup_images,t_unsup_labels = unsup_it.get_next()\n",
    "    \n",
    "    sup_data,sup_label = up.create_per_class_inputs_v2(sup_by_label, sup_per_batch)\n",
    "    sup_it             = sup_data.repeat().make_one_shot_iterator()\n",
    "    sup_it_label       = sup_label.repeat().make_one_shot_iterator()\n",
    "    \n",
    "    # Apply augmentation\n",
    "    def _random_invert(inputs, _):\n",
    "        randu = tf.random_uniform(\n",
    "            shape=[sup_per_batch * NUM_LABELS], minval=0.,\n",
    "            maxval=1.,\n",
    "            dtype=tf.float32)\n",
    "        randu = tf.cast(tf.less(randu, 0.5), tf.float32)\n",
    "        randu = tf.expand_dims(randu, 1)\n",
    "        randu = tf.expand_dims(randu, 1)\n",
    "        randu = tf.expand_dims(randu, 1)\n",
    "        inputs = tf.cast(inputs, tf.float32)\n",
    "        return tf.abs(inputs - 255 * randu)\n",
    "\n",
    "    augmentation_function = _random_invert\n",
    "    \n",
    "    model_func = partial(\n",
    "        arch.svhn_model,\n",
    "        new_shape=new_shape,\n",
    "        augmentation_function=augmentation_function,\n",
    "        img_shape=image_shape,\n",
    "        emb_size=emb_size\n",
    "    )\n",
    "    \n",
    "    model = semisup.SemisupModel(model_func, NUM_LABELS, IMAGE_SHAPE,\n",
    "                                 test_in=tf.placeholder(np.float32, [None] + TEST_SHAPE, 'test_in')\n",
    "                                )\n",
    "    \n",
    "    t_sup_images, t_sup_labels = tf.concat( sup_it.get_next(), 0), tf.concat( sup_it_label.get_next(), 0)\n",
    "    \n",
    "    t_sup_emb   = model.image_to_embedding(t_sup_images)\n",
    "    t_unsup_emb = model.image_to_embedding(t_unsup_images)\n",
    "    \n",
    "    t_sup_logit = model.embedding_to_logit(t_sup_emb)\n",
    "    \n",
    "    visit_weight = apply_envelope(\n",
    "        type = visit_weight_envelope,\n",
    "        step = model.step,\n",
    "        final_weight = visit_weight,\n",
    "        growing_steps = visit_weight_envelope_steps,\n",
    "        delay = visit_weight_envelope_delay\n",
    "    )\n",
    "    \n",
    "    walker_weight = apply_envelope(\n",
    "        type = walker_weight_envelope,\n",
    "        step = model.step,\n",
    "        final_weight = walker_weight,\n",
    "        growing_steps = walker_weight_envelope_steps,\n",
    "        delay = walker_weight_envelope_delay\n",
    "    )\n",
    "    \n",
    "    tf.summary.scalar(\"Weights_Visit\", visit_weight)\n",
    "    tf.summary.scalar(\"Weight_Walker\", walker_weight)\n",
    "    \n",
    "    model.add_logit_loss(t_sup_logit, t_sup_labels, weight=logit_weight)\n",
    "    \n",
    "    #model.add_semisup_loss(t_sup_emb, t_unsup_emb, t_sup_labels, visit_weight=visit_weight, walker_weight=walker_weight)\n",
    "    equality_matrix = tf.equal(tf.reshape(t_sup_labels, [-1, 1]), t_sup_labels)\n",
    "    equality_matrix = tf.cast(equality_matrix, tf.float32)\n",
    "    p_target = (equality_matrix / tf.reduce_sum(\n",
    "        equality_matrix, [1], keepdims=True))\n",
    "\n",
    "    embedding_dim = t_sup_emb.shape[1]\n",
    "    reshaped_semb = tf.reshape( t_sup_emb, [-1, 1, embedding_dim] )\n",
    "    reshaped_uemb = tf.reshape( t_unsup_emb, [-1, 1, embedding_dim] )\n",
    "    \n",
    "    stacked_semb = tf.stack(unsup_batch_size*[t_sup_emb], 1)\n",
    "    stacked_uemb = tf.stack(sup_per_batch*NUM_LABELS*[t_unsup_emb], 1)\n",
    "    \n",
    "    uemb_T = tf.transpose(stacked_uemb, perm=[1,0,2])\n",
    "     \n",
    "    sigma = 0.5\n",
    "    pairwise_dist = (stacked_semb - uemb_T)#, axis=2)\n",
    "    pairwise_norm = tf.norm( pairwise_dist, axis=2)\n",
    "    pairwise_sq   = tf.square(pairwise_norm)\n",
    "    \n",
    "    match_ab   = tf.exp(- tf.divide( pairwise_sq, tf.constant(2*sigma**2, dtype=tf.float32)), name='match_ab')\n",
    "    \n",
    "    p_ab = tf.nn.softmax(match_ab, name='p_ab')\n",
    "    p_ba = tf.nn.softmax(tf.transpose(match_ab), name='p_ba')\n",
    "    p_aba = tf.matmul(p_ab, p_ba, name='p_aba')\n",
    "\n",
    "    model.create_walk_statistics(p_aba, equality_matrix)\n",
    "    \n",
    "    loss_aba = tf.losses.softmax_cross_entropy(\n",
    "        p_target,\n",
    "        tf.log(1e-8 + p_aba),\n",
    "        weights=walker_weight,\n",
    "        scope='loss_aba')\n",
    "    \n",
    "    mab_dt, pab_dt, paba_dt, semb_dt, uemb_dt = tf.gradients([loss_aba], [match_ab, p_ab, p_aba, t_sup_emb, t_unsup_emb])\n",
    "    \n",
    "    model.add_visit_loss(p_ab, visit_weight)\n",
    "\n",
    "    tf.summary.scalar('Loss_aba', loss_aba)\n",
    "    \n",
    "    t_learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate,\n",
    "        model.step,\n",
    "        decay_steps,\n",
    "        decay_factor,\n",
    "        staircase = True\n",
    "    )\n",
    "    \n",
    "    train_op = model.create_train_op(t_learning_rate)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(store_dir, graph)\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/training/mnist-mnistmK/model'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f711b428f30b43ce87e18b8b347baa37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, layout=Layout(width='100%'), max=1002)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-501\n",
      "Time left: 2:24:15s\n",
      "Step: 600\n",
      "[[113   0  16  11   0   1  10   1  20   3]\n",
      " [  6 116  23  12  14   8   8   8  39   0]\n",
      " [  4   1 174  18   1   0   1   5  14   1]\n",
      " [  6   4  20 127   0  15   1   5  26   3]\n",
      " [  1   4  14  14 125   4   8  11  27   9]\n",
      " [  2   0   2  20   2 116   7   2  27   1]\n",
      " [  3   2   8  14   5  15 111   0  19   1]\n",
      " [  1   2  21  13   9   3   0 132  19   5]\n",
      " [  4   1  14  20   1   8   6   6 130   2]\n",
      " [  8   0   9  31  11  14   2  17  23  79]]\n",
      "Test error: 38.85 %\n",
      "Loss: 82.96365356445312\n",
      "\n",
      "saving model - step 600\n",
      "Time left: 1:54:11s\n",
      "Step: 700\n",
      "[[117   1  14  12   0   1   6   2  19   3]\n",
      " [  6 127  17  12  12   9   4  14  33   0]\n",
      " [  5   1 171  19   2   0   0   5  14   2]\n",
      " [  6   4  16 131   0  13   1   6  27   3]\n",
      " [  1   5  12  18 123   5   8  14  23   8]\n",
      " [  2   0   1  22   2 116   6   2  27   1]\n",
      " [  6   3   7  15   7  12 109   1  17   1]\n",
      " [  1   3  19  15   8   3   0 135  19   2]\n",
      " [  6   2  13  21   1   6   6   7 127   3]\n",
      " [  7   2   9  33  16  14   2  17  19  75]]\n",
      "Test error: 38.45 %\n",
      "Loss: 82.9469223022461\n",
      "\n",
      "saving model - step 700\n",
      "Time left: 1:12:55s\n",
      "Step: 800\n",
      "[[116   1  12   9   0   1   8   2  23   3]\n",
      " [  4 126  11   9  14   6   4  17  41   2]\n",
      " [  6   1 159  18   3   0   0   9  20   3]\n",
      " [  5   3  11 132   0  10   0   6  35   5]\n",
      " [  1   3   9  14 123   4   8  15  27  13]\n",
      " [  2   0   1  24   2 111   6   2  30   1]\n",
      " [  7   2   6  15   6   7 110   2  21   2]\n",
      " [  1   2  17  11   8   3   0 138  20   5]\n",
      " [  7   1   8  15   1   4   6   6 141   3]\n",
      " [  4   4   8  28  10  10   2  15  26  87]]\n",
      "Test error: 37.85 %\n",
      "Loss: 82.94602966308594\n",
      "\n",
      "saving model - step 800\n",
      "Time left: 0:35:50s\n",
      "Step: 900\n",
      "[[114   1  13   8   0   1   9   2  24   3]\n",
      " [  4 126  14   7  14   9   4  15  40   1]\n",
      " [  3   1 166  17   3   1   0   7  19   2]\n",
      " [  6   4  10 128   0  17   1   6  32   3]\n",
      " [  1   4  11  13 124  13   8  11  23   9]\n",
      " [  0   0   1  17   3 123   6   2  26   1]\n",
      " [  5   3   7  13   4  16 109   1  19   1]\n",
      " [  1   2  19  10   8   4   0 136  22   3]\n",
      " [  6   2  12  14   1   8   6   4 136   3]\n",
      " [  4   4   7  29  14  17   2  19  27  71]]\n",
      "Test error: 38.35 %\n",
      "Loss: 82.94779205322266\n",
      "\n",
      "saving model - step 900\n",
      "Time left: 0:00:43s\n",
      "Step: 1000\n",
      "[[113   0  13  10   0   1   9   1  25   3]\n",
      " [  4 108  20  10  20   9   5  12  45   1]\n",
      " [  4   1 163  24   2   0   1   5  16   3]\n",
      " [  5   2  10 136   0  13   1   6  30   4]\n",
      " [  1   3  13  17 121   9   8  10  27   8]\n",
      " [  1   0   1  24   2 116   6   1  27   1]\n",
      " [  5   2   7  14   4  13 112   1  19   1]\n",
      " [  1   2  19  15   8   4   0 131  20   5]\n",
      " [  4   1   9  21   1   8   6   3 136   3]\n",
      " [  3   3   6  30  12  15   2  17  27  79]]\n",
      "Test error: 39.25 %\n",
      "Loss: 82.94276428222656\n",
      "\n",
      "saving model - step 1000\n"
     ]
    }
   ],
   "source": [
    "test_images = mnistm_test_images[:2000]\n",
    "test_labels = mnistm_test_labels[:2000]\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "max_steps = 1002\n",
    "f = IntProgress(min=0, max=max_steps, layout= Layout(width=\"100%\")) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "sesh = tf.Session(graph = graph, config=tf.ConfigProto(intra_op_parallelism_threads=2, allow_soft_placement=True))\n",
    "\n",
    "eval_interval = 100\n",
    "\n",
    "p_aba_list = []\n",
    "match_ab_list = []\n",
    "t_sup_emb_list = []\n",
    "t_unsup_emb_list = []\n",
    "\n",
    "model_checkpoint = f\"{log_dir}-{501}\"\n",
    "\n",
    "with sesh as sess:\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=30)\n",
    "    saver.restore(sess, model_checkpoint)\n",
    "    \n",
    "    sess.run(unsup_it.initializer)\n",
    "    \n",
    "    epoch = 0\n",
    "    \n",
    "    for step in range(501, max_steps):\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            loss, _, summaries = sess.run([model.train_loss, train_op, summary_op])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            sess.run(unsup_it.initializer)\n",
    "            epoch += 1\n",
    "            \n",
    "            loss, _, summaries = sess.run([model.train_loss, train_op, summary_op])\n",
    "           \n",
    "        f.value = step\n",
    "        f.description = f\"Ep{epoch}:{step}/{max_steps}\"\n",
    "        if (step) % eval_interval == 0:\n",
    "            \n",
    "            t1 = time.time()\n",
    "            print(f\"Time left: {datetime.timedelta(seconds=int((t1-t0)*(max_steps-step)))}s\")\n",
    "            \n",
    "            print('Step: %d' % step)\n",
    "            test_pred = model.classify(test_images).argmax(-1)\n",
    "            conf_mtx = semisup.confusion_matrix(test_labels, test_pred, NUM_LABELS)\n",
    "            test_err = (test_labels != test_pred).mean() * 100\n",
    "            print(conf_mtx)\n",
    "            print('Test error: %.2f %%' % test_err)\n",
    "            print(f'Loss: {loss}')\n",
    "            print()\n",
    "    \n",
    "            test_summary = tf.Summary(\n",
    "                value=[tf.Summary.Value(\n",
    "                    tag='Test Err', simple_value=test_err)])\n",
    "    \n",
    "            summary_writer.add_summary(summaries, step)\n",
    "            summary_writer.add_summary(test_summary, step)\n",
    "\n",
    "            print(f\"saving model - step {step}\")\n",
    "            saver.save(sess, store_dir, global_step=model.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-1\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-101\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-201\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-301\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-401\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistmK/model-501\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s0.5/model-601\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s0.5/model-701\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s0.5/model-801\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s0.5/model-901\n",
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s0.5/model-1001\n"
     ]
    }
   ],
   "source": [
    "all_steps = list(range(1,1002,100))\n",
    "\n",
    "p_aba_list = []\n",
    "match_ab_list = []\n",
    "p_ab_list = []\n",
    "semb_list = []\n",
    "uemb_list = []\n",
    "slabel_list = []\n",
    "ulabel_list = []\n",
    "\n",
    "for train_step in all_steps:\n",
    "    if train_step <= 501:\n",
    "        model_checkpoint = f\"{log_dir}-{train_step}\"\n",
    "    else:\n",
    "        model_checkpoint = f\"{store_dir}-{train_step}\"\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_checkpoint)\n",
    "        \n",
    "        sess.run(unsup_it.initializer)\n",
    "        \n",
    "        pab, slabel, ulabel, ab, aba, semb, uemb = sess.run([p_ab, t_sup_labels, t_unsup_labels, match_ab, p_aba, t_sup_emb, t_unsup_emb])\n",
    "        p_aba_list.append(aba)\n",
    "        match_ab_list.append(ab)\n",
    "        p_ab_list.append(pab)\n",
    "        semb_list.append(semb)\n",
    "        uemb_list.append(uemb)\n",
    "        slabel_list.append(slabel)\n",
    "        ulabel_list.append(ulabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/training/mnist-mnistK-s40/model-601\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    model_checkpoint = f\"{store_dir}-{601}\"\n",
    "    saver.restore(sess, model_checkpoint)\n",
    "    sess.run(unsup_it.initializer)\n",
    "    \n",
    "    laba, mabgr, pabgr, pabagr, sgr, ugr = sess.run([loss_aba, mab_dt, pab_dt, paba_dt, semb_dt, uemb_dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00528175, -0.00561872, -0.00520749, ...,  0.00059999,\n",
       "         0.00059999,  0.00059999],\n",
       "       [-0.00529621, -0.00560133, -0.00522016, ...,  0.00059999,\n",
       "         0.00059999,  0.00059999],\n",
       "       [-0.00530337, -0.00563986, -0.00519285, ...,  0.00059999,\n",
       "         0.00059999,  0.00059999],\n",
       "       ...,\n",
       "       [ 0.00059999,  0.00059999,  0.00059999, ..., -0.00523213,\n",
       "        -0.00545448, -0.00540969],\n",
       "       [ 0.00059999,  0.00059999,  0.00059999, ..., -0.00523463,\n",
       "        -0.00544493, -0.00540712],\n",
       "       [ 0.00059999,  0.00059999,  0.00059999, ..., -0.00528052,\n",
       "        -0.00549948, -0.00536054]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pabagr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.31671072e-05,  2.22829549e-05,  3.88521403e-06, ...,\n",
       "         3.97997937e-05,  2.60515644e-05, -1.02260929e-05],\n",
       "       [ 3.20304352e-05,  2.11506140e-05,  2.69929842e-06, ...,\n",
       "         3.86981919e-05,  2.49356308e-05, -1.14620943e-05],\n",
       "       [ 3.28725364e-05,  2.18775967e-05,  3.56622877e-06, ...,\n",
       "         3.94613635e-05,  2.56402636e-05, -1.04909595e-05],\n",
       "       ...,\n",
       "       [-8.02024297e-05,  2.86057038e-05, -3.66997119e-05, ...,\n",
       "        -1.15878793e-05, -5.89669944e-05,  2.49999139e-05],\n",
       "       [-7.96724198e-05,  2.90179014e-05, -3.62238206e-05, ...,\n",
       "        -1.11469562e-05, -5.85033558e-05,  2.54311744e-05],\n",
       "       [-8.08653858e-05,  2.87733856e-05, -3.68884212e-05, ...,\n",
       "        -1.14469440e-05, -5.83730289e-05,  2.45909032e-05]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pabgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.51997567e-08,  5.30075681e-08,  1.75683752e-08, ...,\n",
       "         8.28770226e-08,  5.79718034e-08, -1.34038212e-08],\n",
       "       [ 3.70654334e-08,  2.02222559e-08, -1.61795572e-08, ...,\n",
       "         5.50345831e-08,  3.00880885e-08, -4.82694666e-08],\n",
       "       [ 7.75299043e-08,  5.55734836e-08,  2.91538385e-08, ...,\n",
       "         9.63606581e-08,  6.20624547e-08, -3.77756582e-09],\n",
       "       ...,\n",
       "       [-1.53317416e-07,  6.22706153e-08, -6.39316724e-08, ...,\n",
       "        -1.10663221e-08, -9.49409724e-08,  6.75445548e-08],\n",
       "       [-1.96473820e-07,  4.34731184e-08, -8.87426950e-08, ...,\n",
       "        -3.11478203e-08, -1.15436286e-07,  4.21994919e-08],\n",
       "       [-1.54327836e-07,  5.69168535e-08, -7.82367806e-08, ...,\n",
       "        -2.56784300e-08, -1.49675856e-07,  4.48925306e-08]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mabgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.73160100e-08,  1.43034535e-08,  9.16143694e-09, ...,\n",
       "        -1.38655025e-08, -7.06721293e-10, -3.01570182e-08],\n",
       "       [ 3.01983434e-08,  2.41971261e-08, -1.71434849e-08, ...,\n",
       "        -5.27470900e-10,  6.03642292e-09,  4.04439788e-08],\n",
       "       [-1.86297182e-08,  3.54174041e-08,  1.08918679e-08, ...,\n",
       "        -1.76803141e-08, -1.73815522e-08, -2.46733158e-08],\n",
       "       ...,\n",
       "       [ 3.09021608e-08, -4.55078919e-10,  1.29072335e-08, ...,\n",
       "        -1.28435185e-09,  2.12495110e-09,  2.38004425e-08],\n",
       "       [ 1.80136139e-09, -1.01491429e-08,  5.80294923e-09, ...,\n",
       "         1.83752409e-08,  2.34520225e-09,  1.82092719e-09],\n",
       "       [ 2.03308250e-08, -4.96763608e-09,  1.00391846e-08, ...,\n",
       "        -2.35994890e-09,  4.96717723e-10,  1.83357010e-08]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_uemb_list     = []\n",
    "sorted_match_ab_list = []\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "for idx in range(len(uemb_list)):\n",
    "    sorted_uemb_list.append(uemb_list[idx][ulabel_list[idx].argsort()])\n",
    "    sorted_match_ab_list.append(match_ab_list[idx][:,ulabel_list[idx].argsort()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cee1459f49477389c4f1a100930a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=901, description='Train Step', max=1501, min=1, step=100), RadioButtons(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(step, array)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from ipywidgets import IntSlider, RadioButtons\n",
    "from IPython.html.widgets import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_img(array, step):\n",
    "    if array == 0:\n",
    "        a = p_aba_list\n",
    "    elif array == 1:\n",
    "        a = match_ab_list\n",
    "    elif array == 2:\n",
    "        a = sorted_match_ab_list\n",
    "    else:\n",
    "        a = p_ab_list \n",
    "            \n",
    "            \n",
    "    fig = plt.figure()\n",
    "    plt.imshow(a[(step-1)//100])\n",
    "    plt.colorbar()\n",
    "\n",
    "interact(\n",
    "    lambda step, array: plot_img(array, step),\n",
    "    step = IntSlider(\n",
    "        value=901,\n",
    "        min=1,\n",
    "        max=1501,\n",
    "        step=100,\n",
    "        description='Train Step',\n",
    "        continuous_update=True,\n",
    "        orientation='horizontal',\n",
    "    ),\n",
    "    array = RadioButtons(\n",
    "        options=[('p_aba', 0), (\"match_ab\", 1), (\"sorted_match_ab\", 2), (\"p_ab\", 3)],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "\n",
    "my_semb = semb_list[idx]\n",
    "my_uemb = uemb_list[idx]\n",
    "\n",
    "embedding_dim = my_semb.shape[1]\n",
    "\n",
    "reshaped_semb = my_semb.reshape((-1, 1, embedding_dim))\n",
    "reshaped_uemb = my_uemb.reshape((-1, 1, embedding_dim))\n",
    "\n",
    "repeat_semb = np.repeat(reshaped_semb, unsup_batch_size, axis=1)\n",
    "repeat_uemb = np.repeat(reshaped_uemb, sup_per_batch*NUM_LABELS, axis=1)\n",
    "\n",
    "uemb_T = np.transpose(repeat_uemb, axes=[1,0,2])\n",
    "\n",
    "sigma = 40\n",
    "my_match_ab = np.exp(-(np.linalg.norm(repeat_semb - uemb_T, axis=2)**2)/(2*sigma**2))\n",
    "\n",
    "\n",
    "from sklearn.utils.extmath import softmax\n",
    "plt.imshow(softmax(my_match_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "\n",
    "with g2.as_default():\n",
    "    \n",
    "    x = tf.ones([1])\n",
    "    t = tf.gradients([x**2], [x])\n",
    "\n",
    "\n",
    "with tf.Session(graph = g2) as sesh:\n",
    "    print(sesh.run(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
